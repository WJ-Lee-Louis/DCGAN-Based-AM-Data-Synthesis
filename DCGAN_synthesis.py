# -*- coding: utf-8 -*-
"""이미지생성코드.ipynb의 사본

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oHmoqmRKXuIk20nG6FudM7KLXuQ8b7qB
"""

from google.colab import drive
drive.mount('/content/drive')

import os
import glob
from PIL import Image
import random
import numpy as np

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, Subset
from torchvision import transforms
from torchvision.utils import save_image

BASE_DIR = "/content/drive/MyDrive/산업공학종합설계2/AM_Datasets/train"
TYPE1_DIR = os.path.join(BASE_DIR, "type1")
TYPE2_DIR = os.path.join(BASE_DIR, "type2")
OUTPUT_ROOT = "/content/drive/MyDrive/산업공학종합설계2/GAN_outputs_128_best_25"

IMG_SIZE    = 128
LATENT_DIM  = 100
BATCH_SIZE  = 4
N_GENERATE  = 475
N_TRAIN_SAMPLES = 25

EPOCHS_GAN  = 500
LR_GAN      = 2e-4

SEED = 42
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)

class SimpleImageDataset(Dataset):
    def __init__(self, root, img_size=IMG_SIZE):
        self.files = []

        all_paths = glob.glob(os.path.join(root, "*"))
        exts = [".png", ".jpg", ".jpeg"]

        for p in all_paths:
            ext = os.path.splitext(p)[1].lower()
            if ext in exts:
                self.files.append(p)

        self.files = sorted(self.files)

        print(f"[DEBUG] Found {len(self.files)} image files in {root}")

        self.transform = transforms.Compose([
            transforms.Resize((img_size, img_size)),
            transforms.ToTensor(),
            transforms.Normalize([0.5, 0.5, 0.5],
                                 [0.5, 0.5, 0.5]),
        ])

    def __len__(self):
        return len(self.files)

    def __getitem__(self, idx):
        path = self.files[idx]
        img = Image.open(path).convert("RGB")
        img = self.transform(img)
        return img


def create_dataloader(
    dataset_path,
    img_size,
    batch_size,
    n_train_samples=None,
    seed=SEED,
):
    dataset = SimpleImageDataset(dataset_path, img_size=img_size)
    total_imgs = len(dataset)

    if total_imgs == 0:
        raise RuntimeError(
            f"[ERROR] No images found in {dataset_path}. "
            f"경로와 확장자(.png/.jpg/.jpeg) 확인 필요"
        )

    if (n_train_samples is not None) and (n_train_samples < total_imgs):
        rng = np.random.RandomState(seed)
        indices = rng.choice(total_imgs, n_train_samples, replace=False)
        subset = Subset(dataset, indices)
        print(f"[INFO] Using subset: {len(subset)}/{total_imgs} images")
        loader = DataLoader(subset, batch_size=batch_size,
                            shuffle=True, num_workers=0)
    else:
        print(f"[INFO] Using full dataset: {total_imgs} images")
        loader = DataLoader(dataset, batch_size=batch_size,
                            shuffle=True, num_workers=0)

    return loader, total_imgs

class DCGANGenerator128(nn.Module):
    def __init__(self, latent_dim=100, img_size=128, channels=3):
        super().__init__()
        self.latent_dim = latent_dim

        self.net = nn.Sequential(
            # (latent_dim, 1, 1) → (1024, 4, 4)
            nn.ConvTranspose2d(latent_dim, 1024, 4, 1, 0, bias=False),
            nn.BatchNorm2d(1024),
            nn.ReLU(True),

            # (1024, 4, 4) → (512, 8, 8)
            nn.ConvTranspose2d(1024, 512, 4, 2, 1, bias=False),
            nn.BatchNorm2d(512),
            nn.ReLU(True),

            # (512, 8, 8) → (256, 16, 16)
            nn.ConvTranspose2d(512, 256, 4, 2, 1, bias=False),
            nn.BatchNorm2d(256),
            nn.ReLU(True),

            # (256, 16, 16) → (128, 32, 32)
            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False),
            nn.BatchNorm2d(128),
            nn.ReLU(True),

            # (128, 32, 32) → (64, 64, 64)
            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias=False),
            nn.BatchNorm2d(64),
            nn.ReLU(True),

            # (64, 64, 64) → (3, 128, 128)
            nn.ConvTranspose2d(64, channels, 4, 2, 1, bias=False),
            nn.Tanh()  # [-1, 1]
        )

    def forward(self, z):
        z = z.view(z.size(0), self.latent_dim, 1, 1)
        img = self.net(z)
        return img


class DCGANDiscriminator128(nn.Module):
    def __init__(self, img_size=128, channels=3):
        super().__init__()

        layers = [
            # (C, 128, 128) → (64, 64, 64)
            nn.Conv2d(channels, 64, 4, 2, 1, bias=False),
            nn.LeakyReLU(0.2, inplace=True),

            # (64, 64, 64) → (128, 32, 32)
            nn.Conv2d(64, 128, 4, 2, 1, bias=False),
            nn.BatchNorm2d(128),
            nn.LeakyReLU(0.2, inplace=True),

            # (128, 32, 32) → (256, 16, 16)
            nn.Conv2d(128, 256, 4, 2, 1, bias=False),
            nn.BatchNorm2d(256),
            nn.LeakyReLU(0.2, inplace=True),

            # (256, 16, 16) → (512, 8, 8)
            nn.Conv2d(256, 512, 4, 2, 1, bias=False),
            nn.BatchNorm2d(512),
            nn.LeakyReLU(0.2, inplace=True),

            # (512, 8, 8) → (1024, 4, 4)
            nn.Conv2d(512, 1024, 4, 2, 1, bias=False),
            nn.BatchNorm2d(1024),
            nn.LeakyReLU(0.2, inplace=True),

            # (1024, 4, 4) → (1, 1, 1)
            nn.Conv2d(1024, 1, 4, 1, 0, bias=False),
        ]
        self.main = nn.Sequential(*layers)
        self.out_act = nn.Sigmoid()

    def forward(self, x):
        out = self.main(x)
        out = out.view(out.size(0), -1)
        out = self.out_act(out)
        return out

def train_DCGAN(
    dataset_path,
    cls_name,
    exp_root,
    img_size=IMG_SIZE,
    latent_dim=LATENT_DIM,
    n_epochs=EPOCHS_GAN,
    batch_size=BATCH_SIZE,
    lr_G=LR_GAN,
    lr_D=LR_GAN,
    n_generate=N_GENERATE,
    n_train_samples=N_TRAIN_SAMPLES,
):
    exp_dir     = os.path.join(OUTPUT_ROOT, exp_root, cls_name)
    samples_dir = os.path.join(exp_dir, "samples")
    ckpt_dir    = os.path.join(exp_dir, "checkpoints")
    os.makedirs(samples_dir, exist_ok=True)
    os.makedirs(ckpt_dir,    exist_ok=True)

    dataloader, total_imgs = create_dataloader(
        dataset_path,
        img_size,
        batch_size,
        n_train_samples=n_train_samples,
        seed=SEED,
    )

    print(f"\n[{exp_root} | {cls_name}]")
    print(f"- Data path          : {dataset_path}")
    print(f"- #images (total)    : {total_imgs}")
    print(f"- #images (train)    : {min(n_train_samples, total_imgs)}")
    print(f"- Save dir           : {exp_dir}")

    G = DCGANGenerator128(latent_dim=latent_dim,
                          img_size=img_size,
                          channels=3).to(device)
    D = DCGANDiscriminator128(img_size=img_size,
                              channels=3).to(device)

    # ----- Optimizer & Loss -----
    optimizer_G = optim.Adam(G.parameters(), lr=lr_G, betas=(0.5, 0.999))
    optimizer_D = optim.Adam(D.parameters(), lr=lr_D, betas=(0.5, 0.999))
    adversarial_loss = nn.BCELoss().to(device)

    fixed_noise = torch.randn(64, latent_dim, device=device)
    step = 0

    # ----- Training Loop -----
    for epoch in range(1, n_epochs + 1):
        for real_imgs in dataloader:
            real_imgs = real_imgs.to(device)
            bs = real_imgs.size(0)

            # -----------------------------
            #  Discriminator
            # -----------------------------
            valid = torch.ones(bs, 1, device=device)
            fake  = torch.zeros(bs, 1, device=device)

            optimizer_D.zero_grad()

            # Real
            real_scores = D(real_imgs)
            d_real_loss = adversarial_loss(real_scores, valid)

            # Fake
            z = torch.randn(bs, latent_dim, device=device)
            fake_imgs = G(z)
            fake_scores = D(fake_imgs.detach())
            d_fake_loss = adversarial_loss(fake_scores, fake)

            d_loss = (d_real_loss + d_fake_loss) / 2
            d_loss.backward()
            optimizer_D.step()

            # -----------------------------
            #  Generator
            # -----------------------------
            optimizer_G.zero_grad()
            z = torch.randn(bs, latent_dim, device=device)
            gen_imgs = G(z)
            gen_scores = D(gen_imgs)
            g_loss = adversarial_loss(gen_scores, valid)
            g_loss.backward()
            optimizer_G.step()

            if step % 50 == 0:
                print(
                    f"[Epoch {epoch}/{n_epochs}] [Step {step}] "
                    f"D loss: {d_loss.item():.4f} | G loss: {g_loss.item():.4f}"
                )

            if step % 200 == 0:
                G.eval()
                with torch.no_grad():
                    fake_sample = G(fixed_noise).detach().cpu()
                G.train()
                sample_path = os.path.join(
                    samples_dir, f"epoch{epoch:03d}_step{step:06d}.png"
                )
                save_image(fake_sample, sample_path,
                           nrow=8, normalize=True, value_range=(-1, 1))
            step += 1

        torch.save(G.state_dict(), os.path.join(ckpt_dir, f"G_epoch{epoch:03d}.pth"))
        torch.save(D.state_dict(), os.path.join(ckpt_dir, f"D_epoch{epoch:03d}.pth"))

    print(f"[INFO] Training finished for {exp_root} | {cls_name}")
    print("[INFO] Now generating final samples...")

    G.eval()
    final_dir = os.path.join(exp_dir, "samples_final")
    os.makedirs(final_dir, exist_ok=True)

    with torch.no_grad():
        num_batches = (n_generate + 63) // 64
        counter = 0
        for _ in range(num_batches):
            curr_batch = min(64, n_generate - counter)
            if curr_batch <= 0:
                break
            z = torch.randn(curr_batch, latent_dim, device=device)
            gen_imgs = G(z).cpu()
            for j in range(curr_batch):
                img = gen_imgs[j:j+1]
                out_path = os.path.join(final_dir, f"final_{counter:04d}.png")
                save_image(img, out_path,
                           nrow=1, normalize=True, value_range=(-1, 1))
                counter += 1

    print(f"[INFO] Generated {counter} images in {final_dir}")

for cls_name, cls_dir in [("type1", TYPE1_DIR), ("type2", TYPE2_DIR)]:
    train_DCGAN(
        dataset_path=cls_dir,
        cls_name=cls_name,
        exp_root="SET_best_128_25",
        n_epochs=EPOCHS_GAN,
        lr_G=LR_GAN,
        lr_D=LR_GAN,
        n_train_samples=N_TRAIN_SAMPLES,
    )